{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to steal some parts of my original model, but now we want to use MULTIPLE models to give our main output (predicted close) a much more robust \"range\" to the closing price, rather than hinging on a single-point.\n",
    "\n",
    "### Output would be: \"Predicted closing price range of: $435.75 - $439.42\"\n",
    "\n",
    "Given this range, our model can now make a programmatical decision on whether it thinks the stock is going to go up, down, or remain flat. This decision will be a CONFIGURATION as it is entirely dependent on the human's threshhold / tolerance for risk and what they're trying to achieve.\n",
    "\n",
    "### \"I'm interested in making trades when the model predicts a minimum upward swing of 4% on the stock.\"\n",
    "\n",
    "## Can we give ourselves a confidence rating?\n",
    "\n",
    "It would be nice for the model to say \"I'm 90% sure the closing price will fall within the predicted range\" or even better would be a percentage.\n",
    "How do we do this with only a \"1:1\" (row:output) comparison of the data?\n",
    "\n",
    "The real answer is to do a time series model, but right now I'd like to focus on the tools at hand, while I'm learning.\n",
    "\n",
    "To produce a confidence rating, I am able to use the \"mean absolute error\" of the model as it's trained on the training set and compared to the value set... but if I'm training my model on the WHOLE data set, my model should be 100% in-sync with the training set by the time it's finished training. \n",
    "\n",
    "## How do we produce a range?\n",
    "\n",
    "I think attempting to predict the range of ALL the desired features is a good idea. The problem is that none of the data will be dependent on the previous \"time steps\". If we ask the model to chop the data randomly, we're learning based on a moment in time and not on a series of moments.\n",
    "\n",
    "*** THIS IS TOTALLY NOT GOOD MODELING AND I KNOW IT! ***\n",
    "*** THIS IS JUST TO LEARN THE BASICS BEFORE WE GET MORE ROBUST WITH \"TIME SERIES\" MODELING!!! ***\n",
    "*** IT WILL BE OF GENERAL INTEREST IF IT PRODUCES SEMI-ACCURATE RESULTS, THOUGH! ***\n",
    "\n",
    "With this in mind, my gut says running the model randomly 1000 times may produce some consistency or some kind of normal distribution that I can use to define a \"range\" for any given feature.\n",
    "\n",
    "If I capture this data, and plot it, I should see SOME outliers, but I should also hope to see a tight clustering that I can define as the \"range\" for that feature's prediction\n",
    "\n",
    "## 1st model\n",
    "\n",
    "Determine a range for volume based on the high, low, open, and close features. (1000 trains with \"volume\" being the train_y = 1 RANGE PREDICTION)\n",
    "\n",
    "## 2nd model\n",
    "\n",
    "Determine a range for high & low based on volume, open, and close. (train on whole set of data, but make it predict based on the RANGE from 1st model and the previous open and close price. one prediction for the LOW volume, one prediction for the HIGH volume - 2 TOTAL PREDICTIONS)\n",
    "\n",
    "*** now we can say we've done everything we can to provide the FINAL pass with the most accurate predictions possible ***\n",
    "\n",
    "## 3rd model\n",
    "\n",
    "This model will be trained on the whole data set and attempt to predict a single Close price based on the 2 predictions is receives from the 2nd model\n",
    "\n",
    "The TWO predictions we receive from this 3rd model should be taken into consideration alongside the HIGH and LOW predictions of model #2 to attempt to build the final Close price RANGE prediction that a human trader would want to use as a guide for how to trade the next upcoming trading day.\n",
    "\n",
    "Now that we have a north star, let's get coding!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all relevant imports and download CSV from yfinance API call\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from datetime import date\n",
    "\n",
    "ticker = 'AMZN'\n",
    "\n",
    "unsafe_session = requests.session()\n",
    "unsafe_session.verify = False\n",
    "\n",
    "def load_data(tickerSymbols):\n",
    "    yf.download(tickers=tickerSymbols\n",
    "                , session=unsafe_session\n",
    "                ).to_csv(f'./csv/{tickerSymbols}_data.csv')\n",
    "\n",
    "    # load data into DataFrame\n",
    "    return pd.read_csv(f'./csv/{tickerSymbols}_data.csv')\n",
    "\n",
    "def prediction_csv(dataframe, ticker):\n",
    "    dataframe.to_csv(f'./predict-csv/{ticker}_data.csv')\n",
    "\n",
    "def add_line_to_file(file_path, new_line):\n",
    "\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(new_line + \"\\n\")\n",
    "\n",
    "def mean_within_one_std(arr):\n",
    "    \"\"\"Calculates the mean of elements within one standard deviation from the mean.\"\"\"\n",
    "\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "\n",
    "    # Filter elements within one standard deviation\n",
    "    filtered_arr = [x for x in arr if mean - std <= x <= mean + std]\n",
    "\n",
    "    # Calculate mean of filtered array\n",
    "    return filtered_arr\n",
    "\n",
    "# get only data rows by \n",
    "data_rows_only = load_data(ticker).iloc[2:]\n",
    "\n",
    "#shift data, concat() columns, and rename for analyzing data\n",
    "df_shifted = data_rows_only.shift(1)\n",
    "df_shifted.columns = ['Price_prev', 'Close_prev', 'High_prev', 'Low_prev', 'Open_prev', 'Volume_prev']\n",
    "df_combined = pd.concat([data_rows_only.loc[:,], df_shifted], axis=1)\n",
    "df_combined.columns = ['Current Date', 'Current Close', 'Current High', 'Current Low', 'Current Open', 'Current Volume',\n",
    "                       'Day_prev', 'Close_prev', 'High_prev', 'Low_prev', 'Open_prev', 'Volume_prev']\n",
    "\n",
    "print(f\"{len(df_shifted)} rows\")\n",
    "print(df_combined.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build stock model and feature sets\n",
    "stock_model_1 = RandomForestRegressor(random_state=1)\n",
    "stock_features_1 = ['Close_prev', 'High_prev', 'Low_prev', 'Open_prev']\n",
    "### TARGET =  VOLUME ###\n",
    "\n",
    "stock_model_2 = RandomForestRegressor(random_state=1)\n",
    "stock_features_2 = ['Close_prev', 'Open_prev', 'Volume_prev'] # can share the feature set, no problem!\n",
    "### TARGET =  LOW ###\n",
    "\n",
    "stock_model_3 = RandomForestRegressor(random_state=1)\n",
    "### TARGET =  HIGH  ###\n",
    "\n",
    "stock_model_4 = RandomForestRegressor(random_state=1)\n",
    "stock_features_4 = ['Close_prev', 'High_prev', 'Low_prev', 'Open_prev', 'Volume_prev']\n",
    "### TARGET =  CLOSE  ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_closed = df_combined.iloc[len(df_combined)-1]\n",
    "\n",
    "# establish X (rows to analyze) and y (value to predict) variables\n",
    "X1 = df_combined.iloc[1:][stock_features_1]\n",
    "recent_closed_X1 = recent_closed[stock_features_1]\n",
    "y1 = df_combined.iloc[1:]['Current Volume']\n",
    "\n",
    "X2 = df_combined.iloc[1:][stock_features_2] # same feature set\n",
    "y2 = df_combined.iloc[1:]['Current Low']\n",
    "\n",
    "X3 = df_combined.iloc[1:][stock_features_2] # same feature set\n",
    "y3 = df_combined.iloc[1:]['Current High']\n",
    "\n",
    "X4 = df_combined.iloc[1:][stock_features_4]\n",
    "y4 = df_combined.iloc[1:]['Current Close']\n",
    "\n",
    "# global scope variables for data extraction\n",
    "\n",
    "prediction_array = []\n",
    "\n",
    "# let's start the 1000 random prediction loops here:\n",
    "# (this is not \"training\" the model, we are merely producing a sample of data to derive our educated guesses from)\n",
    "\n",
    "for i in range(1000):\n",
    "    # split the training set on each loop\n",
    "    train_X1, val_X1, train_y1, val_y1 = train_test_split(X1, y1)\n",
    "\n",
    "    # fit first model on TRAINING data set, we want as many random configurations of data points analyzed as possible outcomes, hence the looping\n",
    "    # from there, we'll take an average --- (and maybe throw out outliers? we may want to take an average of outcomes that are within 1 standard deviation from the mean)\n",
    "    stock_model_1.fit(train_X1,train_y1)\n",
    "\n",
    "    prediction_1 = stock_model_1.predict([recent_closed_X1])\n",
    "    prediction_array.append(prediction_1[0]) # make a prediction and push it to the array\n",
    "\n",
    "    print(f\"Loop {i} prediction:: {prediction_1[0]}\")\n",
    "\n",
    "prediction_array = mean_within_one_std(prediction_array)\n",
    "\n",
    "low_volume = np.min(prediction_array).astype(np.float64)\n",
    "high_volume = np.max(prediction_array).astype(np.float64)\n",
    "\n",
    "print(\"The range is:\")\n",
    "print(f\"{low_volume} - {high_volume}\")\n",
    "\n",
    "print_txt = f\"FOR TICKER '{ticker}', \\n\\tThe predicted volume range is:{low_volume} - {high_volume}\"\n",
    "\n",
    "add_line_to_file(\"./predictions/multi-predictions-log.txt\", print_txt)\n",
    "\n",
    "# fit last three models on WHOLE data set\n",
    "stock_model_2.fit(X2,y2) # Low model\n",
    "stock_model_3.fit(X3,y3) # high model\n",
    "stock_model_4.fit(X4,y4) # high model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have the models fitted... let's make some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 1000 predictions for good measure, track the MINIMUM and MAXIMUM volume resultss to produce a range\n",
    "\n",
    "# for time, let's use the outputs i got for the 1st model 50 loops:\n",
    "# 27891551.0 - 45309967.0\n",
    "\n",
    "df_2 = pd.DataFrame({'Close_prev': [recent_closed[\"Close_prev\"]], 'Open_prev': [recent_closed[\"Open_prev\"]], 'Volume_prev': [low_volume], })\n",
    "df_3 = pd.DataFrame({'Close_prev': [recent_closed[\"Close_prev\"]], 'Open_prev': [recent_closed[\"Open_prev\"]], 'Volume_prev': [high_volume], })\n",
    "\n",
    "# make two predictions for the HIGH and LOW volume\n",
    "prediction_2 = stock_model_2.predict(df_2)[0]\n",
    "prediction_3 = stock_model_3.predict(df_3)[0]\n",
    "\n",
    "print(f\"Low price is: {prediction_2}\")\n",
    "print(f\"High price is: {prediction_3}\")\n",
    "print_txt = f\"\\tLow price is: {prediction_2}. \\nHigh price is: {prediction_3}.\"\n",
    "\n",
    "add_line_to_file(\"./predictions/multi-predictions-log.txt\", print_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp for logging\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%H:%M %m/%d/%Y\")\n",
    "\n",
    "# Now let's make the very LAST prediction based on this new information!\n",
    "df_4 = pd.DataFrame({'Close_prev': [recent_closed[\"Close_prev\"]], 'High_prev': [prediction_3], 'Low_prev': [prediction_2], \n",
    "                     'Open_prev': [recent_closed[\"Open_prev\"]], 'Volume_prev': [low_volume], })\n",
    "\n",
    "df_5 = pd.DataFrame({'Close_prev': [recent_closed[\"Close_prev\"]], 'High_prev': [prediction_3], 'Low_prev': [prediction_2], \n",
    "                     'Open_prev': [recent_closed[\"Open_prev\"]], 'Volume_prev': [high_volume], })\n",
    "\n",
    "prediction_4 = stock_model_4.predict(df_4)[0]\n",
    "prediction_5 = stock_model_4.predict(df_5)[0]\n",
    "\n",
    "add_line_to_file(\"./predictions/multi-predictions-log.txt\", f\"------------------------\\n\\n{timestamp}:\\n\\n\")\n",
    "\n",
    "if prediction_4 < prediction_5:\n",
    "    print_txt = f\"\\tClose price range for next trading day on '{ticker}' is: {prediction_4} - {prediction_5}\"\n",
    "\n",
    "    add_line_to_file(\"./predictions/multi-predictions-log.txt\", print_txt)\n",
    "    print(print_txt)\n",
    "else:\n",
    "    print_txt = f\"\\tClose price range for next trading day on '{ticker}' is: {prediction_5} - {prediction_4}\"\n",
    "    add_line_to_file(\"./predictions/multi-predictions-log.txt\", print_txt)\n",
    "    print(print_txt)\n",
    "\n",
    "add_line_to_file(\"./predictions/multi-predictions-log.txt\", \"\\n\\n------------------------\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%H:%M %m/%d/%Y\")\n",
    "\n",
    "print(timestamp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
